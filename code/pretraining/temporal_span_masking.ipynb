{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    concatenate_datasets\n",
    ")\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DefaultDataCollator,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME=\"klue/roberta-large\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[MASK]', 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token, tokenizer.mask_token_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset 가져와서 answer masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'document_id', '__index_level_0__', 'answers.answer_start', 'answers.text'],\n",
       "    num_rows: 4192\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=load_from_disk(\"../../data/train_dataset/\")\n",
    "dataset=concatenate_datasets([dataset['train'],dataset['validation']])\n",
    "dataset=dataset.flatten()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미국 상원</td>\n",
       "      <td>미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...</td>\n",
       "      <td>대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?</td>\n",
       "      <td>mrc-1-000067</td>\n",
       "      <td>18293</td>\n",
       "      <td>하원</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사조직관리</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>현대적 인사조직관리의 시발점이 된 책은?</td>\n",
       "      <td>mrc-0-004397</td>\n",
       "      <td>51638</td>\n",
       "      <td>《경영의 실제》</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title                                            context  \\\n",
       "0   미국 상원  미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...   \n",
       "1  인사조직관리  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "\n",
       "                           question            id  document_id answer_text  \\\n",
       "0  대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?  mrc-1-000067        18293          하원   \n",
       "1            현대적 인사조직관리의 시발점이 된 책은?  mrc-0-004397        51638    《경영의 실제》   \n",
       "\n",
       "   answer_text_start  \n",
       "0                235  \n",
       "1                212  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=dataset.to_pandas()\n",
    "\n",
    "df['answer_text']=df['answers.text'].apply(lambda x:x[0])\n",
    "df['answer_text_start']=df['answers.answer_start'].apply(lambda x:x[0])\n",
    "df=df.drop(columns=['answers.text', 'answers.answer_start','__index_level_0__'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bc186351f341cb83c9d57053e07cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_start</th>\n",
       "      <th>offsets</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_start</th>\n",
       "      <th>token_end</th>\n",
       "      <th>salient_answer</th>\n",
       "      <th>label_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미국 상원</td>\n",
       "      <td>미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...</td>\n",
       "      <td>대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?</td>\n",
       "      <td>mrc-1-000067</td>\n",
       "      <td>18293</td>\n",
       "      <td>하원</td>\n",
       "      <td>235</td>\n",
       "      <td>[(0, 0), (0, 2), (3, 5), (5, 6), (7, 9), (10, ...</td>\n",
       "      <td>[0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>하원</td>\n",
       "      <td>[0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사조직관리</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>현대적 인사조직관리의 시발점이 된 책은?</td>\n",
       "      <td>mrc-0-004397</td>\n",
       "      <td>51638</td>\n",
       "      <td>《경영의 실제》</td>\n",
       "      <td>212</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 3), (3, 4), (5, 8), (8, 9...</td>\n",
       "      <td>[0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...</td>\n",
       "      <td>103</td>\n",
       "      <td>108</td>\n",
       "      <td>《 경영의 실제 》</td>\n",
       "      <td>[0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title                                            context  \\\n",
       "0   미국 상원  미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...   \n",
       "1  인사조직관리  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "\n",
       "                           question            id  document_id answer_text  \\\n",
       "0  대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?  mrc-1-000067        18293          하원   \n",
       "1            현대적 인사조직관리의 시발점이 된 책은?  mrc-0-004397        51638    《경영의 실제》   \n",
       "\n",
       "   answer_text_start                                            offsets  \\\n",
       "0                235  [(0, 0), (0, 2), (3, 5), (5, 6), (7, 9), (10, ...   \n",
       "1                212  [(0, 0), (0, 1), (1, 3), (3, 4), (5, 8), (8, 9...   \n",
       "\n",
       "                                           input_ids  token_start  token_end  \\\n",
       "0  [0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...          123        124   \n",
       "1  [0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...          103        108   \n",
       "\n",
       "  salient_answer                                          label_ids  \n",
       "0             하원  [0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...  \n",
       "1     《 경영의 실제 》  [0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "input_ids_list = []\n",
    "token_start_index_list=[]\n",
    "token_end_index_list=[]\n",
    "salient_answer_list=[]\n",
    "labels_list=[]\n",
    "offset_list=[]\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    row['answer_text_end']=row['answer_text_start']+len(row['answer_text'])\n",
    "    \n",
    "    # masking answer\n",
    "    encoded = tokenizer(row['context'], return_offsets_mapping=True,max_length=None)\n",
    "    input_ids=encoded.pop('input_ids')\n",
    "    labels=copy(input_ids)\n",
    "    offsets = encoded.pop('offset_mapping') # token 인덱스 -> 원래 텍스트의 span\n",
    "\n",
    "    # answer_start가 포함된 token의 index를 찾기\n",
    "    token_start_index = 0\n",
    "    while offsets[token_start_index][0] <= row['answer_text_start'] and \\\n",
    "        row['answer_text_start'] >= offsets[token_start_index][1]:\n",
    "        token_start_index += 1\n",
    "\n",
    "    # answer의 끝 index를 찾기\n",
    "    token_end_index = token_start_index # 이전 start 인덱스에서부터 탐색\n",
    "    while offsets[token_end_index][0] <= row['answer_text_end'] and \\\n",
    "        row['answer_text_end'] >= offsets[token_end_index][1]:\n",
    "        token_end_index += 1\n",
    "\n",
    "    # 찾아낸 answer들이 어떻게 tokenize되는지 보기 위해 저장\n",
    "    salient_answer = tokenizer.decode(input_ids[token_start_index:token_end_index])\n",
    "    # answer 부분 masking\n",
    "    input_ids[token_start_index:token_end_index]=[tokenizer.mask_token_id]*(token_end_index-token_start_index)\n",
    "    \n",
    "    offset_list.append(offsets)\n",
    "    token_start_index_list.append(token_start_index)\n",
    "    token_end_index_list.append(token_end_index)\n",
    "    salient_answer_list.append(salient_answer)\n",
    "    input_ids_list.append(input_ids)\n",
    "    labels_list.append(labels)\n",
    "\n",
    "df['offsets']=offset_list\n",
    "df['input_ids']=input_ids_list\n",
    "df['token_start']=token_start_index_list\n",
    "df['token_end']=token_end_index_list\n",
    "df['salient_answer']=salient_answer_list\n",
    "df['label_ids']=labels_list\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_start</th>\n",
       "      <th>offsets</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_start</th>\n",
       "      <th>token_end</th>\n",
       "      <th>salient_answer</th>\n",
       "      <th>label_ids</th>\n",
       "      <th>isSame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미국 상원</td>\n",
       "      <td>미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...</td>\n",
       "      <td>대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?</td>\n",
       "      <td>mrc-1-000067</td>\n",
       "      <td>18293</td>\n",
       "      <td>하원</td>\n",
       "      <td>235</td>\n",
       "      <td>[(0, 0), (0, 2), (3, 5), (5, 6), (7, 9), (10, ...</td>\n",
       "      <td>[0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>하원</td>\n",
       "      <td>[0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사조직관리</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>현대적 인사조직관리의 시발점이 된 책은?</td>\n",
       "      <td>mrc-0-004397</td>\n",
       "      <td>51638</td>\n",
       "      <td>《경영의 실제》</td>\n",
       "      <td>212</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 3), (3, 4), (5, 8), (8, 9...</td>\n",
       "      <td>[0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...</td>\n",
       "      <td>103</td>\n",
       "      <td>108</td>\n",
       "      <td>《 경영의 실제 》</td>\n",
       "      <td>[0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title                                            context  \\\n",
       "0   미국 상원  미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...   \n",
       "1  인사조직관리  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "\n",
       "                           question            id  document_id answer_text  \\\n",
       "0  대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?  mrc-1-000067        18293          하원   \n",
       "1            현대적 인사조직관리의 시발점이 된 책은?  mrc-0-004397        51638    《경영의 실제》   \n",
       "\n",
       "   answer_text_start                                            offsets  \\\n",
       "0                235  [(0, 0), (0, 2), (3, 5), (5, 6), (7, 9), (10, ...   \n",
       "1                212  [(0, 0), (0, 1), (1, 3), (3, 4), (5, 8), (8, 9...   \n",
       "\n",
       "                                           input_ids  token_start  token_end  \\\n",
       "0  [0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...          123        124   \n",
       "1  [0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...          103        108   \n",
       "\n",
       "  salient_answer                                          label_ids  isSame  \n",
       "0             하원  [0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...    True  \n",
       "1     《 경영의 실제 》  [0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...   False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isSame']=df['answer_text']==df['salient_answer']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch=df[df['isSame']==False][['answer_text','salient_answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_start</th>\n",
       "      <th>offsets</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_start</th>\n",
       "      <th>token_end</th>\n",
       "      <th>salient_answer</th>\n",
       "      <th>label_ids</th>\n",
       "      <th>isSame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>신도 (장봉리)</td>\n",
       "      <td>노랑부리백로는 동부아시아의 온대, 우수리, 만주, 중국 동부, 한국 등지에서 주로 ...</td>\n",
       "      <td>섬의 꼭대기 부근에서 노랑부리백로와 공존하는 새는?</td>\n",
       "      <td>mrc-0-002958</td>\n",
       "      <td>28483</td>\n",
       "      <td>괭이갈매기</td>\n",
       "      <td>572</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 4), (4, 5), (5, 6), (6, 7...</td>\n",
       "      <td>[0, 18805, 18442, 2353, 2200, 2259, 6369, 1657...</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td></td>\n",
       "      <td>[0, 18805, 18442, 2353, 2200, 2259, 6369, 1657...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>휴먼센타이패드</td>\n",
       "      <td>카트먼이 다른 학생들은 다 가지고 있는 아이패드가 없지만 있는 척하다가 없다는 사실...</td>\n",
       "      <td>카일의 입과 항문이 연결된 사람은 어느 나라 사람인가?</td>\n",
       "      <td>mrc-1-001793</td>\n",
       "      <td>43615</td>\n",
       "      <td>일본</td>\n",
       "      <td>751</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 3), (3, 4), (5, 7), (8, 1...</td>\n",
       "      <td>[0, 15252, 2615, 2052, 3656, 3767, 2031, 2073,...</td>\n",
       "      <td>374</td>\n",
       "      <td>374</td>\n",
       "      <td></td>\n",
       "      <td>[0, 15252, 2615, 2052, 3656, 3767, 2031, 2073,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>세미놀 전쟁</td>\n",
       "      <td>1818년 3월, 잭슨은 스콧 요새에 군대를 집결시켰다. 이 군대는 800명의 미연...</td>\n",
       "      <td>잘못을 인정하지 않고 반발한 인물은 누구인가?</td>\n",
       "      <td>mrc-0-000331</td>\n",
       "      <td>27774</td>\n",
       "      <td>아버스놋</td>\n",
       "      <td>1280</td>\n",
       "      <td>[(0, 0), (0, 3), (3, 4), (4, 5), (6, 7), (7, 8...</td>\n",
       "      <td>[0, 15236, 2196, 2440, 23, 2429, 16, 16871, 20...</td>\n",
       "      <td>665</td>\n",
       "      <td>665</td>\n",
       "      <td></td>\n",
       "      <td>[0, 15236, 2196, 2440, 23, 2429, 16, 16871, 20...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>첼시 FC</td>\n",
       "      <td>첼시의 홈 유니폼은 파란색을 바탕으로 한다. 그러나 창단 초기의 홈 유니폼은 지금과...</td>\n",
       "      <td>토미 도허티의 지도 아래 첼시 선수들은 무슨 색 양말을 신게 되었는가?</td>\n",
       "      <td>mrc-1-000846</td>\n",
       "      <td>9844</td>\n",
       "      <td>하얀</td>\n",
       "      <td>237</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 3), (4, 5), (6, 9), (9, 1...</td>\n",
       "      <td>[0, 15088, 2079, 1930, 11300, 2073, 18280, 206...</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td></td>\n",
       "      <td>[0, 15088, 2079, 1930, 11300, 2073, 18280, 206...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>마쿠라노소시</td>\n",
       "      <td>≪전쟁론≫ 이전에 전쟁 이론은 대부분 전략 및 전술 중심의 테마로 구성되었으나 ≪전...</td>\n",
       "      <td>현대 사회에서 전쟁의 궁극적인 목적으로 보는 것은?</td>\n",
       "      <td>mrc-0-002681</td>\n",
       "      <td>20507</td>\n",
       "      <td>정치</td>\n",
       "      <td>625</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 3), (3, 4), (4, 5), (6, 8...</td>\n",
       "      <td>[0, 133, 23249, 2570, 3572, 4176, 2170, 4175, ...</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td></td>\n",
       "      <td>[0, 133, 23249, 2570, 3572, 4176, 2170, 4175, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>클레오파트라 3세 필로메토르 소테이라</td>\n",
       "      <td>기원전 142년 퓌스콘은 클레오파트라 2세를 버리고 클레오파트라 3세와 결혼했다. ...</td>\n",
       "      <td>클레오파트라 3세가 후계자를 지명할 수 있도록 유언한 사람은?</td>\n",
       "      <td>mrc-0-000492</td>\n",
       "      <td>25561</td>\n",
       "      <td>퓌스콘</td>\n",
       "      <td>210</td>\n",
       "      <td>[(0, 0), (0, 3), (4, 7), (7, 8), (9, 13), (14,...</td>\n",
       "      <td>[0, 11561, 17205, 2440, 3, 12906, 2168, 2160, ...</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td></td>\n",
       "      <td>[0, 11561, 17205, 2440, 3, 12906, 2168, 2160, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>소드 해변</td>\n",
       "      <td>프랑스 공방전 이후, 영국 총리 윈스턴 처칠은 유럽 대륙과 나치 독일이 점령한 국가...</td>\n",
       "      <td>영국이 오버로드 작전을 수행할 시점에 캉은 어느 나라의 지배를 받고 있었나?</td>\n",
       "      <td>mrc-0-002896</td>\n",
       "      <td>37253</td>\n",
       "      <td>독일</td>\n",
       "      <td>721</td>\n",
       "      <td>[(0, 0), (0, 3), (4, 6), (6, 7), (8, 10), (10,...</td>\n",
       "      <td>[0, 4510, 8722, 2165, 3719, 16, 4353, 4866, 14...</td>\n",
       "      <td>355</td>\n",
       "      <td>355</td>\n",
       "      <td></td>\n",
       "      <td>[0, 4510, 8722, 2165, 3719, 16, 4353, 4866, 14...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>열마디개미속</td>\n",
       "      <td>열마디개미속의 개미들은 탁 트인 곳에 둥지를 만들며, 주식은 어린 식물, 씨앗, 그...</td>\n",
       "      <td>Fire ant의 침에 있는 성분은 무엇인가?</td>\n",
       "      <td>mrc-1-000953</td>\n",
       "      <td>23880</td>\n",
       "      <td>솔레놉신</td>\n",
       "      <td>179</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 3), (3, 4), (4, 5), (5, 6...</td>\n",
       "      <td>[0, 1432, 6295, 2019, 2044, 2354, 2079, 12646,...</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1432, 6295, 2019, 2044, 2354, 2079, 12646,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>클래리티</td>\n",
       "      <td>CLARITY 영상을 적용하는 과정은 조직 샘플로 시작한다. 다음으로 거의 모든 원...</td>\n",
       "      <td>세포 성분과 아크릴 아마이드의 결합을 위해 첨가되는 것은?</td>\n",
       "      <td>mrc-1-000762</td>\n",
       "      <td>43997</td>\n",
       "      <td>열</td>\n",
       "      <td>350</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 3), (3, 5), (5, 6), (6, 7...</td>\n",
       "      <td>[0, 39, 21072, 13607, 2081, 2214, 4729, 2069, ...</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td></td>\n",
       "      <td>[0, 39, 21072, 13607, 2081, 2214, 4729, 2069, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>우르술라</td>\n",
       "      <td>성녀 우르술라와 그녀를 따른 처녀들에 관한 전설은 10세기에 기록된 성인전에서 유래...</td>\n",
       "      <td>훈족이 교황을 만나고 돌아가던 순례자들을 공격한 곳은?</td>\n",
       "      <td>mrc-0-003174</td>\n",
       "      <td>11058</td>\n",
       "      <td>쾰른</td>\n",
       "      <td>489</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 2), (3, 5), (5, 6), (6, 7...</td>\n",
       "      <td>[0, 1268, 2306, 26447, 2072, 2181, 2522, 3821,...</td>\n",
       "      <td>259</td>\n",
       "      <td>259</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1268, 2306, 26447, 2072, 2181, 2522, 3821,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>남세균</td>\n",
       "      <td>원핵식물로 조류에 속하는 이들은 거의가 단세포, 군체 및 실 모양인 다세포체를 이룬...</td>\n",
       "      <td>남조식물의 세포벽을 구성하는 주요 물질은?</td>\n",
       "      <td>mrc-0-002828</td>\n",
       "      <td>7582</td>\n",
       "      <td>뮤코펩티드</td>\n",
       "      <td>772</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 2), (2, 4), (4, 5), (6, 8...</td>\n",
       "      <td>[0, 1478, 2878, 17487, 2200, 10910, 2170, 8770...</td>\n",
       "      <td>437</td>\n",
       "      <td>437</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1478, 2878, 17487, 2200, 10910, 2170, 8770...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>가키자키씨</td>\n",
       "      <td>지금의 가키자키 씨의 시조는 와카사 다케다 씨(若狭武田氏) 또는 난부 씨(南部氏)의...</td>\n",
       "      <td>다케다 노부히로가 통치한 지역은 어디인가?</td>\n",
       "      <td>mrc-1-000995</td>\n",
       "      <td>18605</td>\n",
       "      <td>홋카이도</td>\n",
       "      <td>135</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 3), (4, 5), (5, 6), (6, 8...</td>\n",
       "      <td>[0, 3660, 2079, 543, 2089, 25707, 1370, 2079, ...</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td></td>\n",
       "      <td>[0, 3660, 2079, 543, 2089, 25707, 1370, 2079, ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>프란츠 할더</td>\n",
       "      <td>1939년 봄, 할더는 육군 총참모장으로서 폴란드 침공작전을 입안하기 시작했다. 할...</td>\n",
       "      <td>할더가 히틀러의 공격안을 반대한 결과는?</td>\n",
       "      <td>mrc-0-000561</td>\n",
       "      <td>11517</td>\n",
       "      <td>독일</td>\n",
       "      <td>108</td>\n",
       "      <td>[(0, 0), (0, 4), (4, 5), (6, 7), (7, 8), (9, 1...</td>\n",
       "      <td>[0, 20968, 2440, 1165, 16, 1892, 2320, 2259, 9...</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td></td>\n",
       "      <td>[0, 20968, 2440, 1165, 16, 1892, 2320, 2259, 9...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                            context  \\\n",
       "254               신도 (장봉리)  노랑부리백로는 동부아시아의 온대, 우수리, 만주, 중국 동부, 한국 등지에서 주로 ...   \n",
       "342                휴먼센타이패드  카트먼이 다른 학생들은 다 가지고 있는 아이패드가 없지만 있는 척하다가 없다는 사실...   \n",
       "717                 세미놀 전쟁  1818년 3월, 잭슨은 스콧 요새에 군대를 집결시켰다. 이 군대는 800명의 미연...   \n",
       "1133                 첼시 FC  첼시의 홈 유니폼은 파란색을 바탕으로 한다. 그러나 창단 초기의 홈 유니폼은 지금과...   \n",
       "1188                마쿠라노소시  ≪전쟁론≫ 이전에 전쟁 이론은 대부분 전략 및 전술 중심의 테마로 구성되었으나 ≪전...   \n",
       "1517  클레오파트라 3세 필로메토르 소테이라  기원전 142년 퓌스콘은 클레오파트라 2세를 버리고 클레오파트라 3세와 결혼했다. ...   \n",
       "1684                 소드 해변  프랑스 공방전 이후, 영국 총리 윈스턴 처칠은 유럽 대륙과 나치 독일이 점령한 국가...   \n",
       "2116                열마디개미속  열마디개미속의 개미들은 탁 트인 곳에 둥지를 만들며, 주식은 어린 식물, 씨앗, 그...   \n",
       "3127                  클래리티  CLARITY 영상을 적용하는 과정은 조직 샘플로 시작한다. 다음으로 거의 모든 원...   \n",
       "3381                  우르술라  성녀 우르술라와 그녀를 따른 처녀들에 관한 전설은 10세기에 기록된 성인전에서 유래...   \n",
       "3683                   남세균  원핵식물로 조류에 속하는 이들은 거의가 단세포, 군체 및 실 모양인 다세포체를 이룬...   \n",
       "3996                 가키자키씨  지금의 가키자키 씨의 시조는 와카사 다케다 씨(若狭武田氏) 또는 난부 씨(南部氏)의...   \n",
       "4029                프란츠 할더  1939년 봄, 할더는 육군 총참모장으로서 폴란드 침공작전을 입안하기 시작했다. 할...   \n",
       "\n",
       "                                        question            id  document_id  \\\n",
       "254                 섬의 꼭대기 부근에서 노랑부리백로와 공존하는 새는?  mrc-0-002958        28483   \n",
       "342               카일의 입과 항문이 연결된 사람은 어느 나라 사람인가?  mrc-1-001793        43615   \n",
       "717                    잘못을 인정하지 않고 반발한 인물은 누구인가?  mrc-0-000331        27774   \n",
       "1133     토미 도허티의 지도 아래 첼시 선수들은 무슨 색 양말을 신게 되었는가?  mrc-1-000846         9844   \n",
       "1188                현대 사회에서 전쟁의 궁극적인 목적으로 보는 것은?  mrc-0-002681        20507   \n",
       "1517          클레오파트라 3세가 후계자를 지명할 수 있도록 유언한 사람은?  mrc-0-000492        25561   \n",
       "1684  영국이 오버로드 작전을 수행할 시점에 캉은 어느 나라의 지배를 받고 있었나?  mrc-0-002896        37253   \n",
       "2116                   Fire ant의 침에 있는 성분은 무엇인가?  mrc-1-000953        23880   \n",
       "3127            세포 성분과 아크릴 아마이드의 결합을 위해 첨가되는 것은?  mrc-1-000762        43997   \n",
       "3381              훈족이 교황을 만나고 돌아가던 순례자들을 공격한 곳은?  mrc-0-003174        11058   \n",
       "3683                     남조식물의 세포벽을 구성하는 주요 물질은?  mrc-0-002828         7582   \n",
       "3996                     다케다 노부히로가 통치한 지역은 어디인가?  mrc-1-000995        18605   \n",
       "4029                      할더가 히틀러의 공격안을 반대한 결과는?  mrc-0-000561        11517   \n",
       "\n",
       "     answer_text  answer_text_start  \\\n",
       "254        괭이갈매기                572   \n",
       "342           일본                751   \n",
       "717         아버스놋               1280   \n",
       "1133          하얀                237   \n",
       "1188          정치                625   \n",
       "1517         퓌스콘                210   \n",
       "1684          독일                721   \n",
       "2116        솔레놉신                179   \n",
       "3127           열                350   \n",
       "3381          쾰른                489   \n",
       "3683       뮤코펩티드                772   \n",
       "3996        홋카이도                135   \n",
       "4029          독일                108   \n",
       "\n",
       "                                                offsets  \\\n",
       "254   [(0, 0), (0, 2), (2, 4), (4, 5), (5, 6), (6, 7...   \n",
       "342   [(0, 0), (0, 2), (2, 3), (3, 4), (5, 7), (8, 1...   \n",
       "717   [(0, 0), (0, 3), (3, 4), (4, 5), (6, 7), (7, 8...   \n",
       "1133  [(0, 0), (0, 2), (2, 3), (4, 5), (6, 9), (9, 1...   \n",
       "1188  [(0, 0), (0, 1), (1, 3), (3, 4), (4, 5), (6, 8...   \n",
       "1517  [(0, 0), (0, 3), (4, 7), (7, 8), (9, 13), (14,...   \n",
       "1684  [(0, 0), (0, 3), (4, 6), (6, 7), (8, 10), (10,...   \n",
       "2116  [(0, 0), (0, 1), (1, 3), (3, 4), (4, 5), (5, 6...   \n",
       "3127  [(0, 0), (0, 1), (1, 3), (3, 5), (5, 6), (6, 7...   \n",
       "3381  [(0, 0), (0, 1), (1, 2), (3, 5), (5, 6), (6, 7...   \n",
       "3683  [(0, 0), (0, 1), (1, 2), (2, 4), (4, 5), (6, 8...   \n",
       "3996  [(0, 0), (0, 2), (2, 3), (4, 5), (5, 6), (6, 8...   \n",
       "4029  [(0, 0), (0, 4), (4, 5), (6, 7), (7, 8), (9, 1...   \n",
       "\n",
       "                                              input_ids  token_start  \\\n",
       "254   [0, 18805, 18442, 2353, 2200, 2259, 6369, 1657...          304   \n",
       "342   [0, 15252, 2615, 2052, 3656, 3767, 2031, 2073,...          374   \n",
       "717   [0, 15236, 2196, 2440, 23, 2429, 16, 16871, 20...          665   \n",
       "1133  [0, 15088, 2079, 1930, 11300, 2073, 18280, 206...          122   \n",
       "1188  [0, 133, 23249, 2570, 3572, 4176, 2170, 4175, ...          316   \n",
       "1517  [0, 11561, 17205, 2440, 3, 12906, 2168, 2160, ...          115   \n",
       "1684  [0, 4510, 8722, 2165, 3719, 16, 4353, 4866, 14...          355   \n",
       "2116  [0, 1432, 6295, 2019, 2044, 2354, 2079, 12646,...           98   \n",
       "3127  [0, 39, 21072, 13607, 2081, 2214, 4729, 2069, ...          180   \n",
       "3381  [0, 1268, 2306, 26447, 2072, 2181, 2522, 3821,...          259   \n",
       "3683  [0, 1478, 2878, 17487, 2200, 10910, 2170, 8770...          437   \n",
       "3996  [0, 3660, 2079, 543, 2089, 25707, 1370, 2079, ...           85   \n",
       "4029  [0, 20968, 2440, 1165, 16, 1892, 2320, 2259, 9...           63   \n",
       "\n",
       "      token_end salient_answer  \\\n",
       "254         304                  \n",
       "342         374                  \n",
       "717         665                  \n",
       "1133        122                  \n",
       "1188        316                  \n",
       "1517        115                  \n",
       "1684        355                  \n",
       "2116         98                  \n",
       "3127        180                  \n",
       "3381        259                  \n",
       "3683        437                  \n",
       "3996         85                  \n",
       "4029         63                  \n",
       "\n",
       "                                              label_ids  isSame  \n",
       "254   [0, 18805, 18442, 2353, 2200, 2259, 6369, 1657...   False  \n",
       "342   [0, 15252, 2615, 2052, 3656, 3767, 2031, 2073,...   False  \n",
       "717   [0, 15236, 2196, 2440, 23, 2429, 16, 16871, 20...   False  \n",
       "1133  [0, 15088, 2079, 1930, 11300, 2073, 18280, 206...   False  \n",
       "1188  [0, 133, 23249, 2570, 3572, 4176, 2170, 4175, ...   False  \n",
       "1517  [0, 11561, 17205, 2440, 3, 12906, 2168, 2160, ...   False  \n",
       "1684  [0, 4510, 8722, 2165, 3719, 16, 4353, 4866, 14...   False  \n",
       "2116  [0, 1432, 6295, 2019, 2044, 2354, 2079, 12646,...   False  \n",
       "3127  [0, 39, 21072, 13607, 2081, 2214, 4729, 2069, ...   False  \n",
       "3381  [0, 1268, 2306, 26447, 2072, 2181, 2522, 3821,...   False  \n",
       "3683  [0, 1478, 2878, 17487, 2200, 10910, 2170, 8770...   False  \n",
       "3996  [0, 3660, 2079, 543, 2089, 25707, 1370, 2079, ...   False  \n",
       "4029  [0, 20968, 2440, 1165, 16, 1892, 2320, 2259, 9...   False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['salient_answer']==\"\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temporal expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 8)\n",
      "(13, 20)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in re.finditer(r'기원전 [0-9]+년',\"기원전 123년 그리고 기원전 12년도\"):\n",
    "    print(i.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_patterns=[\n",
    "    r'(\\d{1,4}년\\s*)?\\d{1,2}월\\s*(\\d{1,2}일)?',\n",
    "    r'(기원전|BC)?\\s*[0-9]+\\s*(년|세기)',\n",
    "    r'\\d+\\s*(개)?월',\n",
    "    r'\\d+일',\n",
    "    r'[0-9]+\\s*시간',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "temporal_span_list=[]\n",
    "for i,row in df.iterrows():\n",
    "    temporal_spans=[]\n",
    "    for t_pattern in temporal_patterns:\n",
    "        temporal_spans.extend([match.span() for match in re.finditer(t_pattern,row['context'])])\n",
    "    temporal_span_list.append(temporal_spans)\n",
    "\n",
    "df['temporal_span']=temporal_span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'context', 'question', 'id', 'document_id', 'answer_text',\n",
       "       'answer_text_start', 'offsets', 'input_ids', 'token_start', 'token_end',\n",
       "       'salient_answer', 'label_ids', 'isSame', 'temporal_span'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991f4f6c5871423188389d0dc16cd130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, row in tqdm(df.iterrows()):\n",
    "    for span in row['temporal_span']:\n",
    "        # answer_start가 포함된 token의 index를 찾기\n",
    "        token_start_index = 0\n",
    "        while row['offsets'][token_start_index][0] <= span[0] and \\\n",
    "            span[0] >= row['offsets'][token_start_index][1]:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # answer의 끝 index를 찾기\n",
    "        token_end_index = token_start_index # 이전 start 인덱스에서부터 탐색\n",
    "        while row['offsets'][token_end_index][0] <= span[1] and \\\n",
    "            span[1] >= row['offsets'][token_end_index][1]:\n",
    "            token_end_index += 1\n",
    "\n",
    "        # answer 부분 masking\n",
    "        row['input_ids'][token_start_index:token_end_index]=[tokenizer.mask_token_id]*(token_end_index-token_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "mask_num_list=[]\n",
    "for i,row in df.iterrows():\n",
    "    mask_num=Counter(row['input_ids'])[tokenizer.mask_token_id]\n",
    "    mask_num_list.append(mask_num)\n",
    "df['mask_num']=mask_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5., 247., 262., 291., 208., 192., 197., 157., 164., 161., 176.,\n",
       "        134., 110., 118., 119., 117., 115.,  84.,  93.,  78.,  91.,  78.,\n",
       "         64.,  61.,  70.,  62.,  60.,  55.,  51.,  41.,  40.,  47.,  51.,\n",
       "         30.,  32.,  28.,  33.,  22.,  20.,  23.,  21.,  18.,  20.,  20.,\n",
       "         12.,  15.,  12.,   9.,   8.,   4.,   4.,   9.,   7.,   5.,   8.,\n",
       "          3.,   2.,   3.,   3.,   1.,   2.,   3.,   2.,   4.,   2.,   0.,\n",
       "          0.,   1.,   0.,   1.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "         44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "         55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "         66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "         77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "         88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "         99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "        110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "        121., 122., 123., 124., 125., 126., 127., 128.]),\n",
       " <BarContainer object of 128 artists>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['mask_num'],bins=128,range=(0,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>document_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_text_start</th>\n",
       "      <th>offsets</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_start</th>\n",
       "      <th>token_end</th>\n",
       "      <th>salient_answer</th>\n",
       "      <th>label_ids</th>\n",
       "      <th>isSame</th>\n",
       "      <th>temporal_span</th>\n",
       "      <th>mask_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미국 상원</td>\n",
       "      <td>미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...</td>\n",
       "      <td>대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?</td>\n",
       "      <td>mrc-1-000067</td>\n",
       "      <td>18293</td>\n",
       "      <td>하원</td>\n",
       "      <td>235</td>\n",
       "      <td>[(0, 0), (0, 2), (3, 5), (5, 6), (7, 9), (10, ...</td>\n",
       "      <td>[0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>하원</td>\n",
       "      <td>[0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[(123, 126), (129, 132), (473, 476)]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사조직관리</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>현대적 인사조직관리의 시발점이 된 책은?</td>\n",
       "      <td>mrc-0-004397</td>\n",
       "      <td>51638</td>\n",
       "      <td>《경영의 실제》</td>\n",
       "      <td>212</td>\n",
       "      <td>[(0, 0), (0, 1), (1, 3), (3, 4), (5, 8), (8, 9...</td>\n",
       "      <td>[0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...</td>\n",
       "      <td>103</td>\n",
       "      <td>108</td>\n",
       "      <td>《 경영의 실제 》</td>\n",
       "      <td>[0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...</td>\n",
       "      <td>False</td>\n",
       "      <td>[(43, 49), (194, 200), (336, 341), (402, 407),...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>강희제</td>\n",
       "      <td>강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전...</td>\n",
       "      <td>강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?</td>\n",
       "      <td>mrc-1-000362</td>\n",
       "      <td>5028</td>\n",
       "      <td>백성</td>\n",
       "      <td>510</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 3), (3, 4), (5, 7), (7, 8...</td>\n",
       "      <td>[0, 23814, 2021, 2259, 4021, 2897, 1937, 2207,...</td>\n",
       "      <td>278</td>\n",
       "      <td>279</td>\n",
       "      <td>백성</td>\n",
       "      <td>[0, 23814, 2021, 2259, 4021, 2897, 1937, 2207,...</td>\n",
       "      <td>True</td>\n",
       "      <td>[(468, 474), (477, 481)]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>금동삼존불감</td>\n",
       "      <td>불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든...</td>\n",
       "      <td>11~12세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?</td>\n",
       "      <td>mrc-0-001510</td>\n",
       "      <td>34146</td>\n",
       "      <td>중국</td>\n",
       "      <td>625</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (8, 1...</td>\n",
       "      <td>[0, 14506, 2069, 8081, 2015, 3627, 4506, 2075,...</td>\n",
       "      <td>355</td>\n",
       "      <td>356</td>\n",
       "      <td>중국</td>\n",
       "      <td>[0, 14506, 2069, 8081, 2015, 3627, 4506, 2075,...</td>\n",
       "      <td>True</td>\n",
       "      <td>[(645, 649)]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>계사명 사리구</td>\n",
       "      <td>동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기...</td>\n",
       "      <td>명문이 적힌 유물을 구성하는 그릇의 총 개수는?</td>\n",
       "      <td>mrc-0-000823</td>\n",
       "      <td>47334</td>\n",
       "      <td>4개</td>\n",
       "      <td>30</td>\n",
       "      <td>[(0, 0), (0, 2), (2, 3), (3, 5), (5, 6), (6, 7...</td>\n",
       "      <td>[0, 5165, 2104, 4622, 2042, 2266, 2133, 27135,...</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>4개</td>\n",
       "      <td>[0, 5165, 2104, 4622, 2042, 2266, 2133, 27135,...</td>\n",
       "      <td>True</td>\n",
       "      <td>[(332, 337)]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title                                            context  \\\n",
       "0    미국 상원  미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...   \n",
       "1   인사조직관리  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "2      강희제  강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전...   \n",
       "3   금동삼존불감  불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든...   \n",
       "4  계사명 사리구  동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기...   \n",
       "\n",
       "                                  question            id  document_id  \\\n",
       "0         대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?  mrc-1-000067        18293   \n",
       "1                   현대적 인사조직관리의 시발점이 된 책은?  mrc-0-004397        51638   \n",
       "2           강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?  mrc-1-000362         5028   \n",
       "3  11~12세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?  mrc-0-001510        34146   \n",
       "4               명문이 적힌 유물을 구성하는 그릇의 총 개수는?  mrc-0-000823        47334   \n",
       "\n",
       "  answer_text  answer_text_start  \\\n",
       "0          하원                235   \n",
       "1    《경영의 실제》                212   \n",
       "2          백성                510   \n",
       "3          중국                625   \n",
       "4          4개                 30   \n",
       "\n",
       "                                             offsets  \\\n",
       "0  [(0, 0), (0, 2), (3, 5), (5, 6), (7, 9), (10, ...   \n",
       "1  [(0, 0), (0, 1), (1, 3), (3, 4), (5, 8), (8, 9...   \n",
       "2  [(0, 0), (0, 2), (2, 3), (3, 4), (5, 7), (7, 8...   \n",
       "3  [(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (8, 1...   \n",
       "4  [(0, 0), (0, 2), (2, 3), (3, 5), (5, 6), (6, 7...   \n",
       "\n",
       "                                           input_ids  token_start  token_end  \\\n",
       "0  [0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...          123        124   \n",
       "1  [0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...          103        108   \n",
       "2  [0, 23814, 2021, 2259, 4021, 2897, 1937, 2207,...          278        279   \n",
       "3  [0, 14506, 2069, 8081, 2015, 3627, 4506, 2075,...          355        356   \n",
       "4  [0, 5165, 2104, 4622, 2042, 2266, 2133, 27135,...           19         21   \n",
       "\n",
       "  salient_answer                                          label_ids  isSame  \\\n",
       "0             하원  [0, 3666, 10346, 2252, 4013, 3666, 10450, 12, ...    True   \n",
       "1     《 경영의 실제 》  [0, 11, 5496, 2125, 11984, 11, 4013, 11, 6725,...   False   \n",
       "2             백성  [0, 23814, 2021, 2259, 4021, 2897, 1937, 2207,...    True   \n",
       "3             중국  [0, 14506, 2069, 8081, 2015, 3627, 4506, 2075,...    True   \n",
       "4             4개  [0, 5165, 2104, 4622, 2042, 2266, 2133, 27135,...    True   \n",
       "\n",
       "                                       temporal_span  mask_num  \n",
       "0               [(123, 126), (129, 132), (473, 476)]         7  \n",
       "1  [(43, 49), (194, 200), (336, 341), (402, 407),...        15  \n",
       "2                           [(468, 474), (477, 481)]         6  \n",
       "3                                       [(645, 649)]         3  \n",
       "4                                       [(332, 337)]         4  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4192"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['input_ids'].apply(len)==df['label_ids'].apply(len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'label_ids'],\n",
       "    num_rows: 4192\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_dataset=Dataset.from_pandas(df[['input_ids','label_ids']])\n",
    "masked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f0eecc64294093b2651b013c2023f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'label_ids'],\n",
       "    num_rows: 4068\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset = masked_dataset.map(group_texts, batched=True, num_proc=4)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'label_ids'],\n",
       "        num_rows: 3254\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'label_ids'],\n",
       "        num_rows: 814\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset=lm_dataset.train_test_split(0.2)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_data_collator(features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    batch = {}\n",
    "    \n",
    "    batch[\"input_ids\"] = torch.stack([torch.LongTensor(feature[\"input_ids\"]) for feature in features])\n",
    "    batch[\"labels\"] = torch.stack([torch.LongTensor(feature[\"label_ids\"]) for feature in features])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "model=AutoModelForMaskedLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/training_args.py:1524: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/training_args.py:1543: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case tjddn0402/klue-roberta-large-tsm).\n",
      "  warnings.warn(\n",
      "/opt/ml/level2_nlp_mrc-nlp-03/code/pretraining/klue-roberta-large-tsm is already a clone of https://huggingface.co/tjddn0402/klue-roberta-large-tsm. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/4070 : < :, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 31.75 GiB total capacity; 21.06 GiB already allocated; 104.50 MiB free; 21.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args\u001b[39m=\u001b[39mTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mklue-roberta-large-tsm\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     report_to\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer\u001b[39m=\u001b[39mTrainer(\n\u001b[1;32m     17\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     eval_dataset\u001b[39m=\u001b[39mlm_dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1663\u001b[0m )\n\u001b[0;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1669\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1938\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1940\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1943\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1944\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1945\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1946\u001b[0m ):\n\u001b[1;32m   1947\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:2767\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2766\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2767\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2768\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1121\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(prediction_scores\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1120\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1121\u001b[0m     masked_lm_loss \u001b[39m=\u001b[39m loss_fct(prediction_scores\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvocab_size), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1124\u001b[0m     output \u001b[39m=\u001b[39m (prediction_scores,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 31.75 GiB total capacity; 21.06 GiB already allocated; 104.50 MiB free; 21.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=\"klue-roberta-large-tsm\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    push_to_hub=True,\n",
    "    push_to_hub_model_id='klue-roberta-large-tsm',\n",
    "    hub_strategy='every_save',\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=mlm_data_collator,\n",
    "    train_dataset=lm_dataset['train'],\n",
    "    eval_dataset=lm_dataset['test'],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
